//! FIXME docs/writeup/etc.
use super::Utf8Error;
use core::intrinsics::{const_eval_select, likely, unlikely};

/// Transition table for the Shift-DFA.
// Align to an cache line boundary to reduce the cache footprint. This is just a
// rough approximation, and doesn't really need to be perfect.
#[cfg_attr(target_pointer_width = "64", repr(C, align(128)))]
#[cfg_attr(target_pointer_width = "32", repr(C, align(64)))]
struct DfaTransitions([u32; 256]);

// State IDs we need to reference in the code. These and the transition table
// were generated by small program that drives a solver (FIXME link), which is
// why we're able to use a 32 bit rows rather than the traditional 64 bit.
const ERR: u32 = 0;
const END: u32 = 17;

#[rustfmt::skip]
const DFA: &DfaTransitions = {
    const ILL: u32 = 0b00000000000000000000000000000000;
    const X00: u32 = 0b00000000001000100000000000000000;
    const XC2: u32 = 0b00000000000011000000000000000000;
    const XE0: u32 = 0b00000000000110000000000000000000;
    const XE1: u32 = 0b00000000000010000000000000000000;
    const XED: u32 = 0b00000000001011000000000000000000;
    const XF0: u32 = 0b00000000001100100000000000000000;
    const XF1: u32 = 0b00000000000100000000000000000000;
    const XF4: u32 = 0b00000000001110000000000000000000;
    const X80: u32 = 0b01000001100000000000010001100000;
    const X90: u32 = 0b00001001100000000000010001100000;
    const XA0: u32 = 0b00001000000000000110010001100000;

    &DfaTransitions([
        X00, X00, X00, X00, X00, X00, X00, X00, X00, X00, X00, X00, X00, X00, X00, X00,
        X00, X00, X00, X00, X00, X00, X00, X00, X00, X00, X00, X00, X00, X00, X00, X00,

        X00, X00, X00, X00, X00, X00, X00, X00, X00, X00, X00, X00, X00, X00, X00, X00,
        X00, X00, X00, X00, X00, X00, X00, X00, X00, X00, X00, X00, X00, X00, X00, X00,

        X00, X00, X00, X00, X00, X00, X00, X00, X00, X00, X00, X00, X00, X00, X00, X00,
        X00, X00, X00, X00, X00, X00, X00, X00, X00, X00, X00, X00, X00, X00, X00, X00,

        X00, X00, X00, X00, X00, X00, X00, X00, X00, X00, X00, X00, X00, X00, X00, X00,
        X00, X00, X00, X00, X00, X00, X00, X00, X00, X00, X00, X00, X00, X00, X00, X00,

        X80, X80, X80, X80, X80, X80, X80, X80, X80, X80, X80, X80, X80, X80, X80, X80,
        X90, X90, X90, X90, X90, X90, X90, X90, X90, X90, X90, X90, X90, X90, X90, X90,

        XA0, XA0, XA0, XA0, XA0, XA0, XA0, XA0, XA0, XA0, XA0, XA0, XA0, XA0, XA0, XA0,
        XA0, XA0, XA0, XA0, XA0, XA0, XA0, XA0, XA0, XA0, XA0, XA0, XA0, XA0, XA0, XA0,

        ILL, ILL, XC2, XC2, XC2, XC2, XC2, XC2, XC2, XC2, XC2, XC2, XC2, XC2, XC2, XC2,
        XC2, XC2, XC2, XC2, XC2, XC2, XC2, XC2, XC2, XC2, XC2, XC2, XC2, XC2, XC2, XC2,

        XE0, XE1, XE1, XE1, XE1, XE1, XE1, XE1, XE1, XE1, XE1, XE1, XE1, XED, XE1, XE1,
        XF0, XF1, XF1, XF1, XF4, ILL, ILL, ILL, ILL, ILL, ILL, ILL, ILL, ILL, ILL, ILL,
    ])
};

// Use a generic to help the compiler out some â€” we pass both `&[u8]` and `&[u8;
// CHUNK_LEN]` in here, and would like it to know about the constant.
#[must_use]
#[inline]
fn dfa_run(mut state: u32, chunk: impl AsRef<[u8]>) -> u32 {
    for &byte in chunk.as_ref() {
        state = (DFA.0[byte as usize] as u32) >> (state & 31);
    }
    state & 31
}

// advance the DFA a single step, returning the new masked state. If you have a
// slice you should use `dfa_run` instead; it's usually more efficient.
#[must_use]
#[inline(always)]
const fn dfa_step(state: u32, byte: u8) -> u32 {
    ((DFA.0[byte as usize] as u32) >> (state & 31)) & 31
}

// Note: not trivial to change.
const CHUNK_LEN: usize = 16;

/// Walks through `v` checking that it's a valid UTF-8 sequence,
/// returning `Ok(())` in that case, or, if it is invalid, `Err(err)`.
#[inline]
#[rustc_const_unstable(feature = "str_internals", issue = "none")]
pub(crate) const fn run_utf8_validation(inp: &[u8]) -> Result<(), Utf8Error> {
    const fn validate_utf8_const(inp: &[u8]) -> Result<(), Utf8Error> {
        validate_carefully(inp, 0)
    }

    #[inline]
    fn validate_utf8_rt(inp: &[u8]) -> Result<(), Utf8Error> {
        match inp.len() {
            0 => Ok(()),
            1..=CHUNK_LEN => {
                if is_ascii_small(inp) {
                    Ok(())
                } else {
                    validate_carefully(inp, 0)
                }
            }
            _ => validate_utf8_impl(inp),
        }
    }

    // SAFETY: These are equivalent aside from optimizations.
    unsafe { const_eval_select((inp,), validate_utf8_const, validate_utf8_rt) }
}

/// Validation code which isn't particularly optimized, but can produce an
/// accurate `Utf8Error` (and works in `const`), but
#[inline]
const fn validate_carefully(input: &[u8], mut pos: usize) -> Result<(), Utf8Error> {
    let mut state = END;
    let mut valid_up_to = pos;
    while pos < input.len() {
        state = dfa_step(state, input[pos]);
        match state {
            END => valid_up_to = pos + 1,
            ERR => {
                debug_assert!(pos >= valid_up_to && pos - valid_up_to <= 3);
                let error_len =
                    Some(if pos == valid_up_to { 1 } else { (pos - valid_up_to) as u8 });
                return Err(Utf8Error { valid_up_to, error_len });
            }
            // Keep going
            _ => {}
        }
        pos += 1;
    }
    if state != END { Err(Utf8Error { valid_up_to, error_len: None }) } else { Ok(()) }
}

#[inline]
fn validate_utf8_impl(inp: &[u8]) -> Result<(), Utf8Error> {
    let mut state = END;
    let mut pos = 0;
    let (chunks, tail) = inp.as_chunks::<CHUNK_LEN>();
    if !chunks.is_empty() {
        let mut last_state = state;
        let mut chunk_iter = chunks.iter();
        while let Some(chunk) = chunk_iter.next() {
            if state == END && all_ascii_chunk(chunk) {
                let skipped = skip_ascii_chunks(&mut chunk_iter);
                pos += skipped * CHUNK_LEN;
            } else {
                state = dfa_run(state, chunk);
                if unlikely(state == ERR) {
                    break;
                }
                last_state = state;
            }
            pos += core::mem::size_of_val(chunk);
        }
        if unlikely(state == ERR) {
            return Err(utf8_find_error(inp, pos, last_state != END));
        }
    }
    // Did we leave the optimized loop in the middle of a UTF-8 sequence?
    let was_mid_char = state != END;
    debug_assert!(state != ERR);
    if !tail.is_empty() {
        // Check and early return if the last CHUNK_LEN bytes were all ASCII. The
        // motivation here is to avoid bringing the DFA table into the cache for
        // pure ASCII.
        //
        // 1. avoid touching the DFA table for pure-ASCII input.
        // 2. add a branch into the `dfa_run` inner loop.
        //
        // So we check and see if the last CHUNK_LEN bytes were all ASCII. This does
        // compare with a few bytes that we've already processed, but handling
        // that is not required for correctness (and doing so seems ot hurt
        // performance)
        if !was_mid_char && inp.len() >= CHUNK_LEN && tail.len() < CHUNK_LEN {
            use crate::convert::TryFrom;
            let range = (inp.len() - CHUNK_LEN)..inp.len();
            debug_assert!(range.contains(&pos), "{:?}", (range, pos));
            if all_ascii_chunk(<&[u8; CHUNK_LEN]>::try_from(&inp[range]).unwrap()) {
                return Ok(());
            }
        }

        state = dfa_run(state, tail);
    }

    if likely(state == END) {
        return Ok(());
    }
    let (index, backup) =
        if state == ERR { (inp.len() - tail.len(), was_mid_char) } else { (inp.len(), true) };
    Err(utf8_find_error(inp, index, backup))
}

#[inline]
fn backup_not_yet_invalid(inp: &[u8], mut pos: usize) -> usize {
    debug_assert!(!inp.is_empty() && inp.get(..pos).is_some());
    while pos != 0 {
        pos -= 1;
        let is_cont = (inp[pos] & 0b1100_0000) == 0b1000_0000;
        if !is_cont {
            break;
        }
    }
    pos
}

#[cold]
fn utf8_find_error(input: &[u8], mut pos: usize, backup: bool) -> Utf8Error {
    debug_assert!(!input.is_empty());
    if backup {
        pos = backup_not_yet_invalid(input, pos);
    }
    validate_carefully(input, pos).unwrap_err()
}

#[inline]
fn skip_ascii_chunks(s: &mut core::slice::Iter<'_, [u8; CHUNK_LEN]>) -> usize {
    let mut i = 0;
    let initial_slice = s.as_slice();
    while let Some(c) = s.next() {
        if !all_ascii_chunk(c) {
            break;
        }
        i += 1;
    }
    *s = initial_slice[i..].iter();
    i
}

#[inline]
fn all_ascii_chunk(s: &[u8; CHUNK_LEN]) -> bool {
    // Sadly, `core::simd` currently does not compile very efficiently on some
    // targets (all the targets without simd, and some of the targets with it).
    //
    // It's also somewhat untested on others, so out of an abundance of caution
    // we avoid it on any target that isn't both:
    // - Known to support it efficiently.
    // - Actually something we'd use and test on in the versions of libcore we
    //   ship.
    const SIMD_ASCII_TEST: bool = cfg!(any(
        all(any(target_arch = "x86_64", target_arch = "x86"), target_feature = "sse2",),
        all(target_arch = "aarch64", target_feature = "neon"),
    ));

    if SIMD_ASCII_TEST {
        use crate::simd::*;
        // Workaround for <https://github.com/rust-lang/portable-simd/issues/321> :(
        let simd_chunk = Simd::<u8, CHUNK_LEN>::from_array(*s);
        if cfg!(target_arch = "aarch64") {
            simd_chunk.reduce_max() < 0x80
        } else {
            const ALL_HI: Simd<u8, CHUNK_LEN> = Simd::from_array([0x80; CHUNK_LEN]);
            const ZERO: Simd<u8, CHUNK_LEN> = Simd::from_array([0; CHUNK_LEN]);
            (simd_chunk & ALL_HI).simd_eq(ZERO).all()
        }
    } else {
        // On targets where `core::simd` doesn't compile to efficient code we
        // manually do the equivalent using u64-based SWAR using u64. Using u64 and
        // not `usize` here seems better on 32 bit which have 64 bit register
        // access, but ends up just being an extra unroll step on ones which don't
        // (so no worse, and possibly still better).
        type SwarWord = u64;
        const WORD_BYTES: usize = core::mem::size_of::<SwarWord>();
        const _: () = assert!((CHUNK_LEN % WORD_BYTES) == 0 && CHUNK_LEN != 0);
        let (arr, rem) = s.as_chunks::<WORD_BYTES>();
        debug_assert!(rem.is_empty() && !arr.is_empty());
        let mut combined = 0;
        for word_bytes in arr {
            combined |= SwarWord::from_ne_bytes(*word_bytes);
        }
        const ALL_HI: SwarWord = SwarWord::from_ne_bytes([0x80; WORD_BYTES]);
        (combined & ALL_HI) == 0
    }
}

#[inline]
fn is_ascii_small(s: &[u8]) -> bool {
    // LLVM seems to get pretty aggressive if we use a loop here, even if we
    // check the length first, which can end up having some pretty disasterous
    // impacts on performance, seemingly due to inlining(?). In any case.
    //
    // Note that doing this for many sizes
    //
    // It ends up causing performance problems (probably due to lower
    // willingness to inline. Instead of that, we handle a small number of
    // len-ranges by doing reads that intentionally overlap for some of the
    // slice lengths. Note that going overboard here will result in branch
    // prediction issues, so this is intentionally minimal -- just enough to
    // handle lengths up to `CHUNK_LEN` without pain
    match s.len() {
        // Actually handled in caller,
        0 => true,
        1..=3 => {
            // Note: If `a`, `b`, and `c` are all ASCII bytes, then `a | b | c`
            // will be too.
            let all_bytes_ored = s[0] | s[s.len() / 2] | s[s.len() - 1];
            (all_bytes_ored & 0x80) == 0
        }
        4..=16 => {
            // SAFETY: `off..(off + 4)` should be in bounds for `n`.
            #[inline(always)]
            unsafe fn read32_unchecked(n: &[u8], off: usize) -> u32 {
                debug_assert!(n.get(off..(off + core::mem::size_of::<u32>())).is_some());
                // SAFETY: requirements passed on to caller
                unsafe { n.as_ptr().add(off).cast::<u32>().read_unaligned() }
            }
            // SAFETY: All these reads are guaranteed to be in-bounds for all
            // `s.len()` values in the between 4..=16 range. Sadly, the compiler
            // doesn't seem to be able to remove bounds checks on expressions
            // involving `mid_round_down` (no matter how I phrase it), so we need
            // the unsafe.
            let all_u32_ored = unsafe {
                let mid_round_down = (s.len() / 2) & !3;
                let tail = s.len() - 4;
                read32_unchecked(s, 0)
                    | read32_unchecked(s, mid_round_down)
                    | read32_unchecked(s, tail - mid_round_down)
                    | read32_unchecked(s, tail)
            };
            (all_u32_ored & 0x80808080) == 0
        }
        _ => false,
    }
}
